{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook, we are building a machine learning model to predict the housing market values ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries\n",
    "\n",
    "First we import all the libraries to load and augment the data. We will also import the libraries that are used to encode our data and to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pycaret.time_series import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data\n",
    "\n",
    "Next, we load and display the raw data file to check what kind of data we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transaction unique identifier</th>\n",
       "      <th>price</th>\n",
       "      <th>date of transfer</th>\n",
       "      <th>property type</th>\n",
       "      <th>old/new</th>\n",
       "      <th>duration</th>\n",
       "      <th>town/city</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>ppdcategory type</th>\n",
       "      <th>record status - monthly file only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{81B82214-7FBC-4129-9F6B-4956B4A663AD}</td>\n",
       "      <td>25000</td>\n",
       "      <td>1995-08-18 00:00</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>OLDHAM</td>\n",
       "      <td>OLDHAM</td>\n",
       "      <td>GREATER MANCHESTER</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{8046EC72-1466-42D6-A753-4956BF7CD8A2}</td>\n",
       "      <td>42500</td>\n",
       "      <td>1995-08-09 00:00</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>GRAYS</td>\n",
       "      <td>THURROCK</td>\n",
       "      <td>THURROCK</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{278D581A-5BF3-4FCE-AF62-4956D87691E6}</td>\n",
       "      <td>45000</td>\n",
       "      <td>1995-06-30 00:00</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGHBRIDGE</td>\n",
       "      <td>SEDGEMOOR</td>\n",
       "      <td>SOMERSET</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{1D861C06-A416-4865-973C-4956DB12CD12}</td>\n",
       "      <td>43150</td>\n",
       "      <td>1995-11-24 00:00</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>BEDFORD</td>\n",
       "      <td>NORTH BEDFORDSHIRE</td>\n",
       "      <td>BEDFORDSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{DD8645FD-A815-43A6-A7BA-4956E58F1874}</td>\n",
       "      <td>18899</td>\n",
       "      <td>1995-06-23 00:00</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>WAKEFIELD</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489343</th>\n",
       "      <td>{4C4EE000-291A-1854-E050-A8C063054F34}</td>\n",
       "      <td>175000</td>\n",
       "      <td>2017-02-20 00:00</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489344</th>\n",
       "      <td>{4C4EE000-291B-1854-E050-A8C063054F34}</td>\n",
       "      <td>586945</td>\n",
       "      <td>2017-02-15 00:00</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>WETHERBY</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489345</th>\n",
       "      <td>{4C4EE000-291C-1854-E050-A8C063054F34}</td>\n",
       "      <td>274000</td>\n",
       "      <td>2017-02-24 00:00</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "      <td>L</td>\n",
       "      <td>HUDDERSFIELD</td>\n",
       "      <td>KIRKLEES</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489346</th>\n",
       "      <td>{4C4EE000-291D-1854-E050-A8C063054F34}</td>\n",
       "      <td>36000</td>\n",
       "      <td>2017-02-22 00:00</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>HALIFAX</td>\n",
       "      <td>CALDERDALE</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22489347</th>\n",
       "      <td>{4C4EE000-291E-1854-E050-A8C063054F34}</td>\n",
       "      <td>145000</td>\n",
       "      <td>2017-03-03 00:00</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>LEEDS</td>\n",
       "      <td>WEST YORKSHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22489348 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   transaction unique identifier   price  date of transfer  \\\n",
       "0         {81B82214-7FBC-4129-9F6B-4956B4A663AD}   25000  1995-08-18 00:00   \n",
       "1         {8046EC72-1466-42D6-A753-4956BF7CD8A2}   42500  1995-08-09 00:00   \n",
       "2         {278D581A-5BF3-4FCE-AF62-4956D87691E6}   45000  1995-06-30 00:00   \n",
       "3         {1D861C06-A416-4865-973C-4956DB12CD12}   43150  1995-11-24 00:00   \n",
       "4         {DD8645FD-A815-43A6-A7BA-4956E58F1874}   18899  1995-06-23 00:00   \n",
       "...                                          ...     ...               ...   \n",
       "22489343  {4C4EE000-291A-1854-E050-A8C063054F34}  175000  2017-02-20 00:00   \n",
       "22489344  {4C4EE000-291B-1854-E050-A8C063054F34}  586945  2017-02-15 00:00   \n",
       "22489345  {4C4EE000-291C-1854-E050-A8C063054F34}  274000  2017-02-24 00:00   \n",
       "22489346  {4C4EE000-291D-1854-E050-A8C063054F34}   36000  2017-02-22 00:00   \n",
       "22489347  {4C4EE000-291E-1854-E050-A8C063054F34}  145000  2017-03-03 00:00   \n",
       "\n",
       "         property type old/new duration     town/city            district  \\\n",
       "0                    T       N        F        OLDHAM              OLDHAM   \n",
       "1                    S       N        F         GRAYS            THURROCK   \n",
       "2                    T       N        F    HIGHBRIDGE           SEDGEMOOR   \n",
       "3                    T       N        F       BEDFORD  NORTH BEDFORDSHIRE   \n",
       "4                    S       N        F     WAKEFIELD               LEEDS   \n",
       "...                ...     ...      ...           ...                 ...   \n",
       "22489343             S       N        F         LEEDS               LEEDS   \n",
       "22489344             D       N        F      WETHERBY               LEEDS   \n",
       "22489345             D       N        L  HUDDERSFIELD            KIRKLEES   \n",
       "22489346             T       N        F       HALIFAX          CALDERDALE   \n",
       "22489347             T       N        F         LEEDS               LEEDS   \n",
       "\n",
       "                      county ppdcategory type  \\\n",
       "0         GREATER MANCHESTER                A   \n",
       "1                   THURROCK                A   \n",
       "2                   SOMERSET                A   \n",
       "3               BEDFORDSHIRE                A   \n",
       "4             WEST YORKSHIRE                A   \n",
       "...                      ...              ...   \n",
       "22489343      WEST YORKSHIRE                A   \n",
       "22489344      WEST YORKSHIRE                A   \n",
       "22489345      WEST YORKSHIRE                A   \n",
       "22489346      WEST YORKSHIRE                A   \n",
       "22489347      WEST YORKSHIRE                A   \n",
       "\n",
       "         record status - monthly file only  \n",
       "0                                        A  \n",
       "1                                        A  \n",
       "2                                        A  \n",
       "3                                        A  \n",
       "4                                        A  \n",
       "...                                    ...  \n",
       "22489343                                 A  \n",
       "22489344                                 A  \n",
       "22489345                                 A  \n",
       "22489346                                 A  \n",
       "22489347                                 A  \n",
       "\n",
       "[22489348 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(\".\\\\Datasets\\\\price_paid_records.csv\")\n",
    "data.columns = data.columns.str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data augmentation\n",
    "\n",
    "**3.1 Dropping `transaction unique identifier`, `property type`, `old/new`, `duration`, `town/city`, `district`, `county`, `ppdcategory type` and `record status - monthly file only`**\n",
    "\n",
    "For this model we decided to drop the classes `transaction unique identifier`, `property type`, `old/new`, `duration`, `town/city`, `district`, `county`, `ppdcategory type` and `record status - monthly file only`. \n",
    "\n",
    "These columns were dropped because the model only gets the date to base its predictions on and no other classes.\n",
    "\n",
    "We also decided to drop all columns containing null-values. This is because our dataset contained 2 columns, the column to base its predictions on and the column which contained the predictions. When 1 of the 2 is incomplete, the model can't be trained.\n",
    "\n",
    "Lastly we sorted the columns by date of transfer to ensure the model got the data in chronological order to make its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['transaction unique identifier', 'property type', 'old/new', 'duration', 'town/city', 'district', 'county', 'ppdcategory type', 'record status - monthly file only'])\n",
    "data = data.dropna()  # Handle missing values, if any\n",
    "data = data.sort_values(by='date of transfer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Grouping the dataset by month and year**\n",
    "\n",
    "Next up, we converted the values of the `date of transfer` class to datetime values. We did this so that we could then take the month and year for each value. We then created a new class called `yearmonth`. This class contained the string values of the month and year, seperated by a `'-'`. \n",
    "\n",
    "Lastly we grouped the data by the `yearmonth` class. This means that it takes the sum of all values where the `yearmonth` values are the same. It then puts this summ in the according value of the `yearmonth` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of prices for each day\n",
    "data['date of transfer'] = pd.to_datetime(data['date of transfer'])\n",
    "data['yearmonth'] =  data[\"date of transfer\"].dt.year.astype(str) + '-' + data[\"date of transfer\"].dt.month.astype(str).str.zfill(2)\n",
    "data = data.groupby('yearmonth')['price'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Splitting the dataset in train and test data**\n",
    "\n",
    "**3.3.1 Creating the train dataset**\n",
    "\n",
    "We then divided this data in training and testing data. We used the first 215 rows as training data. This equals ~80% of the complete dataset. We used the upper 215 rows so that the model could use the prices of the previous data as a reference.\n",
    "\n",
    "We then saved this data to the subset_80.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "subset_data = data.head(215)\n",
    "subset_file_path = '.\\\\Datasets\\\\subset_80.csv'\n",
    "subset_data.to_csv(subset_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.2 Creating the testing dataset**\n",
    "\n",
    "We use the data from row 215 to the end as test data. This equals 20% of the complete dataset. \n",
    "\n",
    "We then save this data to the test_20.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_data = data.iloc[215:]\n",
    "test_file_path = '.\\\\Datasets\\\\test_20.csv'\n",
    "test_data.to_csv(test_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Sagemaker model\n",
    "\n",
    "The following code was entered in AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Importing the libraries**\n",
    "\n",
    "First we import the libraries used for loading the data, processing the data and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Loading the data**\n",
    "\n",
    "First we define the bucket and the file that needs to be loaded. Then we convert all the different columns to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "bucket_name = \"ukhouseholding\"\n",
    "file_key = \"subset_80.csv\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "url = s3.generate_presigned_url(\n",
    "    ClientMethod=\"get_object\",\n",
    "    Params={\"Bucket\": bucket_name, \"Key\": file_key},\n",
    "    ExpiresIn=3600  # URL expires in 1 hour\n",
    ")\n",
    "data = pd.read_csv(url)\n",
    "data.columns = data.columns.str.lower()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['transaction unique identifier', 'property type', 'old/new', 'duration', 'town/city', 'district', 'county', 'ppdcategory type', 'record status - monthly file only'])\n",
    "data = data.dropna()  # Handle missing values, if any\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date of transfer'] = pd.to_datetime(data['date of transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"price\"] = data[\"price\"].astype(float)\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.drop(columns=[\"price\"])  # Features\n",
    "y = data[\"price\"]                 # Target\n",
    "\n",
    "# Combine the features and target into a single DataFrame for training\n",
    "train_data = pd.concat([y, X], axis=1)\n",
    "\n",
    "# Save the training data to a local CSV file\n",
    "train_data.to_csv(\"train_data.csv\", index=False)\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_prefix = \"train/train_data.csv\"\n",
    "s3_client.upload_file(\"train_data.csv\", bucket_name, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ukhouseholding\"  # Replace with your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"xgboost\", session.boto_region_name, version=\"1.5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p3.2xlarge\",  # GPU instance type\n",
    "    output_path=f\"s3://{bucket_name}/ukhouseholding-xgboost\",  # Output path in S3\n",
    "    sagemaker_session=session,\n",
    "    base_job_name=\"ukhouseholding-xgboost-job\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost.set_hyperparameters(\n",
    "    objective=\"reg:squarederror\",  # Binary classification\n",
    "    num_round=100,            # Number of training rounds\n",
    "    max_depth=5,              # Example hyperparameter\n",
    "    eta=0.2,                  # Learning rate\n",
    "    gamma=4,                  # Minimum loss reduction\n",
    "    subsample=0.8,            # Subsample ratio of training instances\n",
    "    colsample_bytree=0.8      # Subsample ratio of columns\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = TrainingInput(\n",
    "    s3_data=f\"s3://{bucket_name}/{s3_prefix}\",  # S3 path to training data\n",
    "    content_type=\"text/csv\"     # Data format\n",
    ")\n",
    "try:\n",
    "    xgboost.fit({\"train\": train_input})\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "try:\n",
    "    xgboost.fit({\"train\": train_input})\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Check if the job exists\n",
    "job_name = xgboost.latest_training_job.name\n",
    "response = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact_s3_uri = f\"s3://ukhouseholding/ukhouseholding-xgboost/ukhouseholding-xgboost-job-2024-11-27-08-27-55-012/output/model.tar.gz\"\n",
    "\n",
    "# Parse bucket and key from the URI\n",
    "parsed_uri = model_artifact_s3_uri.replace(\"s3://\", \"\").split(\"/\")\n",
    "bucket_name = parsed_uri[0]\n",
    "key = \"/\".join(parsed_uri[1:])\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Download the model artifact locally\n",
    "local_file_path = \"model.tar.gz\"  # Specify the desired local file name\n",
    "s3.download_file(bucket_name, key, local_file_path)\n",
    "\n",
    "print(f\"Model saved locally as {local_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
